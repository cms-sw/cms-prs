{"additions": 83, "auther_ref": "dev/CMSSW_11_3_X/onnx-gpu", "auther_sha": "7cc7fb0bc4d9c7a6807f3bd5e95723230d48cac0", "author": "hqucms", "body": "This PR adds the GPU support for ONNXRuntime. The built library still runs on CPU by default (thus current applications in CMSSW are unaffected), while GPU inference can be enabled if needed (see [example](https://www.onnxruntime.ai/docs/reference/execution-providers/CUDA-ExecutionProvider.html)).\r\n\r\nA few changes needed:\r\n - a small modification on `cuda` is needed (namely, keeping `libcudart_static.a`) for cmake to detect `nvcc` correctly\r\n - `cudnn` is added as a dependency. Note that generally downloading `cudnn` needs NVIDIA Developer Program Membership, though direct download link w/o authentication exists (and used here). Experts should probably double check if  the way we distribute it complies with its [SLA](https://docs.nvidia.com/deeplearning/cudnn/sla/index.html).\r\n\r\n\r\nAlso I am not sure if this will compile on `ppc64le` or `aarch64` (though `cudnn` exists for them).\r\n\r\nFYI @riga @mialiu149", "branch": "IB/CMSSW_11_3_X/master", "changed_files": 5, "closed_at": "1617943679", "comments": 74, "commits": 8, "created_at": "1617192408", "deletions": 1, "labels": ["allow-hqucms", "externals-approved", "fully-signed", "orp-approved", "tests-approved"], "merge_commit_sha": "f6c50ee997cf958fe12b8694b714210b4be352f2", "merged_at": "1617943678", "merged_by": "cmsbuild", "number": 6776, "release-notes": [], "review_comments": 13, "state": "closed", "title": "Add GPU support for ONNXRuntime", "updated_at": "1617943679", "user": "hqucms"}