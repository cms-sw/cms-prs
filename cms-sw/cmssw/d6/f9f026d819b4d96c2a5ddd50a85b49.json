{"additions": 3756, "auther_ref": "flexible_ca_final", "auther_sha": "efb43a0ad710606355c09c2cb09dd2afb5dbb5e7", "author": "AdrianoDee", "body": "This PR proposes a general restructring of the Alpaka implementaion of the CA based pixel tracking. Two here would be to make the CA a bit more flexible, relying less on the `TrackerTraits` and reading the geometry ad runtime from the `TrackerGeometry` in a configurable way, and a bit more lightweight, redesigning the containers to be average-sized rather than max-sized.\r\n\r\nThe three updates here have many overlaps and sinergies but may be taken separately.\r\n\r\n# Developments\r\n\r\n## CA Structures\r\n\r\nIn the CA few structures (defined in `RecoTracker/PixelSeeding/plugins/alpaka/CAStructures.h`) have a fixed size containers to hold the intermediate results. These are:\r\n\r\n1. `OuterHitOfCellContainer`: an array keeping the index of a cell (uint32_t). One per hit. Keeps track of the cells that have that his as the outer one.\r\n2. `CellNeighbors`: an array keeping the index of a cell (uint32_t). One per cell (==maxNumberOfCells). Keeps track of cells connected through the outer hit.\r\n3. `CellTracks`: an array keeping the index of a tuple (uint32_t/uint16_t). One per cell. Keeps track of the tuples to which a cell belong. Mostly to remove duplicates. \r\n\r\nand they are sized on the maximum number of possible association for each cells/track. The current numbers were estimated using TTbar PU samples before Run3 start. \r\n\r\nThe idea here is to just move all these structures to be sized with the average per element using `OneToManyAssoc{Sequential|RandomAccess}` sizable at runtime (allocating the need buffers for the storage and the offsets) and so we can pass the averages at config level:\r\n\r\n1. HitToCell  -> `device_hitToCell_`; with `nOnes = nHits` and `nMany = avgCellsPerHit * nHits`\r\n2. CellToCell -> `device_cellToNeighbors_`, with `nOnes = maxCells` and `nMany = avgCellsPerCell * maxCells`\r\n3. CellToTrack -> `device_cellToTracks_`; with `nOnes = maxCells` and `nMany = avgTracksPerCell * maxCells`\r\n\r\nW.r.t the fixed size approach we need a couple of extra things:\r\n\r\n- A count step to size the offsets, added in the already existing kernels replacing the various push_back.\r\n- A structure to hold the pairings between count and fill when it was not already there (e.g. for the Cells we can recycle what we have): a dummy `CACoupleSoA`.\r\n- A fill step in an extra kernel, generic for all the histograms: `Kernel_fillGenericCouple`.\r\n\r\n### Container Sizes\r\n\r\nFind in https://adriano.web.cern.ch/ca_geometry/containers/ the plots for all the container sizes:\r\n\r\n- stats for single quantities: `nHits`, `nOuterHits` (hits excluding barrel 1), `nCells`, `nTracks`, `nTrips` (a cell attached to another cell);\r\n- averages for:\r\n   - number of cells per outer hits: `nCells_vs_nOuterHits`;\r\n   - number of trips for each cell (so number of cell neighbors): `nTrips_vs_nCells_avg`. For the variant with `nTrips_vs_maxDoublets_avg` the denominator is the fixed size for cells we get from the `maxDoublets` parameter.\r\n   - number of tracks per cell (so the number of track using a cell):  `nCellTracks_vs_nCells_avg`. For the variant with `nCellTracks_vs_maxDoublets` the denominator is the same as above.\r\n- the trends for `nCells` and `nTracks` vs the number of hits, with a fit.  \r\n\r\nAn example here. \"wp\" stands for the working point selected for the given scenario.\r\n\r\n![image](https://github.com/user-attachments/assets/ecd0fefc-100a-4ca9-a00c-f4fdd551f128)\r\n\r\n**N.B. the phase2 quads and trips have the same trends for cells since the graph is the same.**\r\n\r\n### Euristical Sizes for Doublets and Tracks\r\n\r\nThe PR propose also to allow to define euristically the container sizes (`maxNumeberOfDoublets` and `maxNumberOfTuples`) via a `TFormula` that could be useful. At the moment I haven't run any proper test for the impact of this update on the memory usage but the fact that the number of hits, cells and tracks show a non negligible range for consecutive events seems to point to the fact that making the `maxNumeberOfDoublets` dependant on the number of hits may be beneficial for a production setup.\r\n\r\nFor run `Run2024F`, `Run=383631` and `LS=476` `EphemeralHLTPhysics` data:\r\n\r\n![image](https://github.com/user-attachments/assets/dd1a53f3-ac22-4149-ae5b-13795df9ad2f)\r\n\r\nThe trends show also that we can leverage on a functional dependency between nHits and {nTracks|nCells} (~quadratical, as one would expect) for any of the scenarios:\r\n\r\n![image](https://github.com/user-attachments/assets/f58a72de-1b3d-4283-91b0-d9b777b52096)\r\n\r\n(Run3 trips on MC and HLT pp on data overlaps nicely).\r\n\r\nFor example running on pp data (from `Run2024F`) one can easily fit the number of cells with the number of hits giving some safety margin.\r\n\r\n![image](https://github.com/user-attachments/assets/5c6b7ff3-dd09-46fa-acdd-12bddbf7d343)\r\n\r\nSamples used:\r\n\r\n- HIon : data `/store/hidata/HIRun2024B/HIEphemeralHLTPhysics/RAW/v1/000/388/305/00000/d8b13b7d-a94e-4b1f-9aae-bd86836a0459.root`;\r\n- HIon Hyet: MC private `HydjetQMinBias_5020GeV+2023HIRP`;\r\n- HLT pp: data `Run2024F`, Run`383631` and `ls0476` `EphemeralHLTPhysics` data;\r\n- Run3 quads/trips: MC `/store/relval/CMSSW_14_1_0_pre2/RelValTTbar_14TeV/GEN-SIM-DIGI-RAW/PU_140X_mcRun3_2024_realistic_v7_STD_2024_PU-v1/2580000/`;\r\n- Phase2 quads/trips: MC `/RelValTTbar_14TeV/CMSSW_15_0_0_pre1-PU_141X_mcRun4_realistic_v3_STD_Run4D110_PU-v1/GEN-SIM-DIGI-RAW`.\r\n\r\n- `hlt_hion` data \r\n- **N.B.**: for HLT pp on `EphemeralHLTPhysics` data for run `383631` the current limit (`512*1024`) was too low for ~0.32% of the events (over the 10000 used). This does imply any crash, we just stop pushing new doublets.\r\n\r\n## CAGeometry\r\n\r\nA new `ESProduct` (`CAGeometry`) is introduced that holds:\r\n\r\n- `phiCuts`, `minZ`, `maxZ`, `maxR` for doublets; \r\n- the graph (now harcoded in the `SimplePixelTopology`);\r\n- the module numberings, read directly from the `TrackerGeometry`.\r\n- the `dcaCut` and `CAThetaCut`, also expanded to be one per layer useful for future tuning (especially if including the stript detector);\r\n\r\n## Pixel DataFormats SoA \r\n\r\nThe Track and TrackingRecHit SoAs were templated with the `TrackerTraits` mostly to hold the helper histograms and for fixed size arrays (for the number of hits per track or modules). This could be simplified levaraging on the `Portable{Host|Device}MultiCollection`. \r\n\r\n- to remove the Pixel Reco DQM paths from the menu since we could write the SoA to ROOT (that would allow to solve https://its.cern.ch/jira/browse/CMSHLT-3147); \r\n- a couple of test to write and read the hits and tracks SoAs has been added;\r\n- to greatly simplify all the modules that downstream consume the SoAs and that are heavily templated just for the inputs (while doing exactly the same thing);\r\n- to integrate in an easier way non-pixel hits in the CA chain (e.g. strip hits);\r\n\r\n\r\n## Miscs\r\n\r\nI took the chance also to do some clean up here and there:\r\n- removing from the chain but leaving the definition in the code of the `AverageGeometry` actually never used. It can be easily re-enabled if needed;\r\n- a fix to `SimplePixelTopology` numbering for Phase2 modules that was affecting the cluster doublet cuts;\r\n- removed `idealConditions` flag that was changing the cluster cut based on the pixel barrel side (only for Run3). It has never been used and no beneficial effect was found ([studies were done in late 2021 and the efficiency was degradated](https://its.cern.ch/jira/browse/CMSHLT-2187)).\r\n- avoiding to have the `CPE` borught around in the chaing just for a single call to the `FrameSoA` that has been moved in the `CAGeometry`;\r\n- remove the limit to the number of vertices for HI conditions (set to `32` instead of standard `256`). This lead to crashes documented in https://github.com/cms-sw/cmssw/issues/46693. This would need anyway to be investigatged since, also in `master`, there are HI events (with noPU) for which we reconstruct >100 vertices.\r\n\r\n# Performance and Physics Studies\r\n\r\nNo changes to physics performance observed (as expected) for:\r\n\r\n- Run3 pixel-only tracking ([quads](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/run3_quads/plots_pixel_pixel/effandfakePtEtaPhi.png), [trips](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/run3_trips/));\r\n- [Run3 HLT](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/hlt_plots_pp_run3/);\r\n- Phase2 pixel-only tracking ([quads](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/phase2_quads/),[trips](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/phase2_trips/));\r\n- HI pixel-only tracking ([quads](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/hi_plots/)). \r\n\r\nSmall fluctuations visible for Phase2 and HI, I haven't found anything strange or a reason for them. They might be the \"usual\" irreproducibilities. \r\n\r\nPosting here a couple of examples for the records\r\n\r\n![image](https://github.com/user-attachments/assets/8dac8c40-bfd1-401b-9730-7ab30aade10e)\r\n\r\n![image](https://github.com/user-attachments/assets/9d5b9e71-39aa-4f69-8d50-6add4c0d60b6)\r\n\r\n----\r\n\r\n### pp HLT\r\n\r\nPerformance measured on `devfu-c2b03-44-01` running `/frozen/2024/2e34/v1.4/CMSSW_14_1_X/HLT` (adapted to be compatible with `15_0_0_pre2` and the PR) on ~10k events from `Run2024I`, Run`386593` and LS `94` `EphemeralHLTPhysics` data.\r\n\r\n----\r\n#### Througput and timing\r\n\r\nThe throughput is basically untouched\r\n\r\nthis PR:\r\n\r\n```\r\nRunning 3 times over 10000 events with 8 jobs, each with 32 threads, 24 streams and 1 GPUs\r\n   543.1    0.1 ev/s (9700 events, 98.1% overlap)\r\n   538.2    0.1 ev/s (9700 events, 99.0% overlap)\r\n   543.2    0.1 ev/s (9700 events, 97.4% overlap)\r\n --------------------\r\n   541.5    2.8 ev/s\r\n```\r\n\r\nmaster:\r\n\r\n```\r\nRunning 3 times over 10000 events with 8 jobs, each with 32 threads, 24 streams and 1 GPUs\r\n   539.9    0.1 ev/s (9700 events, 98.5% overlap)\r\n   540.6    0.1 ev/s (9700 events, 98.9% overlap)\r\n   540.7    0.1 ev/s (9700 events, 98.4% overlap)\r\n --------------------\r\n   540.4    0.5 ev/s\r\n```\r\n\r\nAs for the event timings find [here](https://adiflori.web.cern.ch/adiflori/circles/web/piechart.php?local=false&dataset=ca_geometry%2Fref_jsons%2Fpid1966878_ref&resource=time_real&colours=default&groups=hlt&data_name=data&show_labels=true&threshold=0) all the piecharts. Posting here two of them (for `real_time`) for the records. The average timings measured are (for the 8 jobs x 3 times):\r\n\r\n- master:  `488.48  4.08 ms/ev`\r\n- this PR: `487.72  4.40 ms/ev`\r\n\r\n----\r\n#### Memory\r\n\r\nAll the memory plots are under https://adriano.web.cern.ch/ca_geometry/memory/.\r\n\r\nThe memory usage for 8 jobs x 32 threads x 24 streams is reduced by ~47%. \r\n\r\n[//]: # \"Note tha the 'dead' time between the jobs is increased while the plateau duration should be unchanged. This could be related to the new ESProducer introduced taking care of all the copies at the beginning? I imagine this may be neglected in production? Sueggestions are welcome!\"\r\n[//]: # \"The two trends are plotted so that the rise to the first plateau of the two jobs overlaps (to appreciate the same lenght of the plateau itself).\"\r\n\r\n![hlt_pp_mem_32t24s8j](https://github.com/user-attachments/assets/634419c5-1221-4ed2-a793-9f5086f5846b)\r\n----\r\n\r\n### Phase2\r\n\r\nPerformance measured on a TTbar D110 PU Run4 RelVal sample (EDM input):\r\n\r\n- `/RelValTTbar_14TeV/CMSSW_15_0_0_pre1-PU_141X_mcRun4_realistic_v3_STD_Run4D110_PU-v1/GEN-SIM-DIGI-RAW`\r\n\r\n----\r\n\r\n#### Througput and timing\r\n\r\nThe optimal setup I found for `master` is 2 jobs with 8 threads and 8 streams. With 3 jobs we go out of memory and with 12 or 16 threads the througput is the same, just the memory increases. So I took this as the *baseline* for the comparisons. Here I'm running quadruplets only since for triplets the memory occupancy is very similar. This is a consequences of the fact that doublets related containers are vastly dominating and that, at the moment, they are the same for quads and trips given the same cell graph.\r\n\r\nthis PR:\r\n\r\n```\r\nRunning 3 times over 1300 events with 2 jobs, each with 8 threads, 8 streams and 1 GPUs\r\n    77.3    0.0 ev/s (1000 events, 99.6% overlap)\r\n    77.0    0.0 ev/s (1000 events, 99.6% overlap)\r\n    77.2    0.0 ev/s (1000 events, 99.5% overlap)\r\n --------------------\r\n    77.2    0.2 ev/s\r\n```\r\n\r\nmaster:\r\n\r\n```\r\nRunning 3 times over 1300 events with 2 jobs, each with 8 threads, 8 streams and 1 GPUs\r\n    70.1    0.1 ev/s (1000 events, 99.7% overlap)\r\n    69.4    0.1 ev/s (1000 events, 99.4% overlap)\r\n    69.2    0.1 ev/s (1000 events, 98.7% overlap)\r\n --------------------\r\n    69.6    0.5 ev/s  \r\n```\r\n\r\n----\r\n\r\n#### Memory\r\n\r\nIn terms of memory the effect is even more important that for Run3 HLT with a reduction of ~70% in a configuration with 2 jobs 8 threads and 8 streams (`8t8s2j`). \r\n\r\n![phase2_memory_throughput_quads_reference](https://github.com/user-attachments/assets/61607a91-a78f-4115-8f14-df4721b94bcf)\r\n\r\nThe same for `12t12s2j` that is not really beneficial for the througput and that is almost filling the two T4 available (for `master`). Also, having more memory available configurations with more jobs may be tested brining almost a factor 2 to max throughput. \r\n\r\n![phase2_memory_throughput_quads](https://github.com/user-attachments/assets/07e1c0f9-04a2-45f5-b86c-d6b40dd8ad5b)\r\n\r\n\r\n\r\n-----\r\n\r\n### HIon\r\n\r\n(thanks to Soohwan for the informations and the samples to set this up)\r\n\r\nAt the moment the HIon menu runs only the pixel local reconstruction on GPU since the pixel track reco is too heavy on the GPU memory. The performance here are mesured:\r\n- on MinBias events from `/store/hidata/HIRun2024B/HIEphemeralHLTPhysics/RAW/v1/000/388/305/00000/d8b13b7d-a94e-4b1f-9aae-bd86836a0459.root`, converted to raw; \r\n- with `/dev/CMSSW_14_2_0/HIon/V11` menu:\r\n    - as is in master (`RefCpu`);\r\n    - as is in master turning on the GPU pixel track reco in Alpaka (`Ref`);\r\n    - modified for this PR with the pixel track reco running on GPU in Alpaka and the max number of cells fixed to the current threshold (`DevFixed`).\r\n\r\nIf I understood well and the configuration stayed the same we currently run with 8 jobs 8 threads and 8 streams (see https://its.cern.ch/jira/browse/CMSHLT-2951). For a full GPU menu (`Ref`) the best I could fit in two T4 is a setup with `8t8s2j`. Running the same setup with this PR the memory usage is reduced by ~72% (with a +70% in througput). \r\n\r\n![hlt_hion_memory_throughput_fixed](https://github.com/user-attachments/assets/ba143949-82a8-4866-9811-b5e8d633e211)\r\n\r\nGiven the lighter memory footprint we can push a bit the full GPU HI menu reaching the same througput w.r.t. to the `RefCpu` setup (`16j16s8j`) with `12t12s4j`. And can go up to +240% it with `16t16s8j` (the maximum I could get). \r\n\r\n![hlt_hion_memory_throughput_fixed_best](https://github.com/user-attachments/assets/ac170ca0-9186-4d41-9408-a0dbedbf9b74)\r\n\r\nThe HI runs seems also a good candidate to test the euristical sizes. For example or run `HIRun2024B`, `Run=388305` and `LS=123` `EphemeralHLTPhysics` data, if we plot the number of cells, hits or tracks from consecutive events, we see:\r\n\r\n![run388305_HIEphemeralHLTPhysics_stats](https://github.com/user-attachments/assets/bc0a159f-4d59-4f84-96c0-c800bbdc8db9)\r\n\r\nFor cells we go from 1e4 to 1e6 (for non zero values). And we can fit the number of cells vs the number of hits:\r\n\r\n![hlt_hionVsHits](https://github.com/user-attachments/assets/02070189-0eaa-4dbb-911c-abdfd57811fb)\r\n\r\nUsing this cut for cells we can reach up to more than three times the current througput with `16t16s16j` (while keeping a good margin on the max memory available). Going above (with e.g. `16t16s20j`) is just increasing the memory occupation while keeping the same througput. \r\n\r\n![image](https://github.com/user-attachments/assets/e259d39e-a207-4fdf-bcaf-6d603cc500a6)\r\n\r\n", "branch": "master", "changed_files": 100, "comments": 2, "commits": 25, "created_at": "1742211150", "deletions": 2236, "labels": ["reconstruction-pending", "dqm-pending", "hlt-pending", "geometry-pending", "pending-signatures", "tests-pending", "orp-pending", "code-checks-pending", "heterogeneous-pending", "tracking", "trk", "changes-dataformats"], "milestone": "CMSSW_15_1_X", "number": 47611, "release-notes": [], "review_comments": 0, "state": "open", "title": "A More Flexible And Lightweight CA", "updated_at": "1742211535", "user": "AdrianoDee"}