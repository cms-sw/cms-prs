{"additions": 3757, "auther_ref": "flexible_ca_final", "auther_sha": "7f77624a5edaf84979203f225aa6eff38cbda8ce", "author": "AdrianoDee", "body": "This PR proposes a general restructring of the Alpaka implementaion of the CA based pixel tracking. The idea here would be to make the CA:\r\n- a bit more flexible, relying less on the `TrackerTraits` and reading the geometry at runtime from the `TrackerGeometry` in a configurable way. This would greatly simplify the inclusion of new layers (e.g. strips);\r\n- a bit more lightweight (on memory), redesigning the containers to be average-sized rather than max-sized.\r\n\r\nThe three updates here have many overlaps and sinergies but may be taken separately (if really needed).\r\n\r\n# Developments\r\n\r\n## CA Structures\r\n\r\nIn the CA few structures (defined in `RecoTracker/PixelSeeding/plugins/alpaka/CAStructures.h`) have a fixed size containers to hold the intermediate results. These are:\r\n\r\n1. `OuterHitOfCellContainer`: an array keeping the index of a cell (uint32_t). One per hit. Keeps track of the cells that have that his as the outer one.\r\n2. `CellNeighbors`: an array keeping the index of a cell (uint32_t). One per cell (==maxNumberOfCells). Keeps track of cells connected through the outer hit.\r\n3. `CellTracks`: an array keeping the index of a tuple (uint32_t/uint16_t). One per cell. Keeps track of the tuples to which a cell belong. Mostly to remove duplicates. \r\n\r\nand they are sized on the maximum number of possible association for each cells/track. The current numbers were estimated using TTbar PU samples before Run3 start. \r\n\r\nThe idea here is to just move all these structures to be sized with the average per element using `OneToManyAssoc{Sequential|RandomAccess}` sizable at runtime (allocating the need buffers for the storage and the offsets) and so we can pass the averages at config level:\r\n\r\n1. HitToCell  -> `device_hitToCell_`; with `nOnes = nHits` and `nMany = avgCellsPerHit * nHits`\r\n2. CellToCell -> `device_cellToNeighbors_`, with `nOnes = maxCells` and `nMany = avgCellsPerCell * maxCells`\r\n3. CellToTrack -> `device_cellToTracks_`; with `nOnes = maxCells` and `nMany = avgTracksPerCell * maxCells`\r\n\r\nW.r.t the fixed size approach we need a couple of extra things:\r\n\r\n- A count step to size the offsets, added in the already existing kernels replacing the various push_back.\r\n- A structure to hold the pairings between count and fill when it was not already there (e.g. for the Cells we can recycle what we have): a dummy `CACoupleSoA`.\r\n- A fill step in an extra kernel, generic for all the histograms: `Kernel_fillGenericCouple`.\r\n\r\n### Container Sizes\r\n\r\nFind in https://adriano.web.cern.ch/ca_geometry/containers/ the plots for all the container sizes:\r\n\r\n- stats for single quantities: `nHits`, `nOuterHits` (hits excluding barrel 1), `nCells`, `nTracks`, `nTrips` (a cell attached to another cell);\r\n- averages for:\r\n   - number of cells per outer hits: `nCells_vs_nOuterHits`;\r\n   - number of trips for each cell (so number of cell neighbors): `nTrips_vs_nCells_avg`. For the variant with `nTrips_vs_maxDoublets_avg` the denominator is the fixed size for cells we get from the `maxDoublets` parameter.\r\n   - number of tracks per cell (so the number of track using a cell):  `nCellTracks_vs_nCells_avg`. For the variant with `nCellTracks_vs_maxDoublets` the denominator is the same as above.\r\n- the trends for `nCells` and `nTracks` vs the number of hits, with a fit.  \r\n\r\nAn example here. \"wp\" stands for the working point selected for the given scenario.\r\n\r\n![image](https://github.com/user-attachments/assets/ecd0fefc-100a-4ca9-a00c-f4fdd551f128)\r\n\r\n**N.B. the phase2 quads and trips have the same trends for cells since the graph is the same.**\r\n\r\n### Euristical Sizes for Doublets and Tracks\r\n\r\nThe PR propose also to allow to define euristically the container sizes (`maxNumeberOfDoublets` and `maxNumberOfTuples`) via a `TFormula`. At the moment I haven't run any proper test for the impact of this update on the memory usage for pp conditions. But the fact that the number of hits, cells and tracks show a wide span for consecutive events seems to point to the fact that making the `maxNumeberOfDoublets` dependent on the number of hits may be beneficial for a production setup (in which we run on consecutive events in parallel).\r\n\r\nFor run `Run2024F`, `Run=383631` and `LS=476` `EphemeralHLTPhysics` data:\r\n\r\n![image](https://github.com/user-attachments/assets/dd1a53f3-ac22-4149-ae5b-13795df9ad2f)\r\n\r\nThe trends show also that we can leverage on a functional dependency between nHits and {nTracks|nCells} (~quadratical, as one would expect) for any of the scenarios:\r\n\r\n![image](https://github.com/user-attachments/assets/f58a72de-1b3d-4283-91b0-d9b777b52096)\r\n\r\n(Run3 trips on MC and HLT pp on data overlaps nicely).\r\n\r\nFor example running on pp data (from `Run2024F`) one can easily fit the number of cells with the number of hits giving some safety margin.\r\n\r\n![image](https://github.com/user-attachments/assets/5c6b7ff3-dd09-46fa-acdd-12bddbf7d343)\r\n\r\nSamples used:\r\n\r\n- HIon : data `/store/hidata/HIRun2024B/HIEphemeralHLTPhysics/RAW/v1/000/388/305/00000/d8b13b7d-a94e-4b1f-9aae-bd86836a0459.root`;\r\n- HIon Hyet: MC private `HydjetQMinBias_5020GeV+2023HIRP`;\r\n- HLT pp: data `Run2024F`, Run`383631` and `ls0476` `EphemeralHLTPhysics` data;\r\n- Run3 quads/trips: MC `/store/relval/CMSSW_14_1_0_pre2/RelValTTbar_14TeV/GEN-SIM-DIGI-RAW/PU_140X_mcRun3_2024_realistic_v7_STD_2024_PU-v1/2580000/`;\r\n- Phase2 quads/trips: MC `/RelValTTbar_14TeV/CMSSW_15_0_0_pre1-PU_141X_mcRun4_realistic_v3_STD_Run4D110_PU-v1/GEN-SIM-DIGI-RAW`.\r\n\r\n- `hlt_hion` data from densely populated `HIRun2024B`  events (run `388305`).\r\n\r\n**N.B. for HLT pp on `EphemeralHLTPhysics` data for run `383631` the current limit (`512*1024`) was too low for ~0.32% of the events (over the 10000 used). This does imply any crash, we just stop pushing new doublets. **\r\n\r\n## CAGeometry\r\n\r\nA new `CAGeometry` object is introduced that holds:\r\n\r\n- `phiCuts`, `minZ`, `maxZ`, `maxR` for doublets; \r\n- the graph (now harcoded in the `SimplePixelTopology`);\r\n- the module numberings, read directly from the `TrackerGeometry`.\r\n- the `dcaCut` and `CAThetaCut`, also expanded to be one per layer useful for future tuning (especially if including the stript detector);\r\n\r\n## Pixel DataFormats SoA \r\n\r\nThe Track and TrackingRecHit SoAs were templated with the `TrackerTraits` mostly to hold the helper histograms and for fixed size arrays (for the number of hits per track or modules). This could be simplified levaraging on the `Portable{Host|Device}MultiCollection`. \r\n\r\n- to remove the Pixel Reco DQM paths from the menu since we could write the SoA to ROOT (that would allow to solve https://its.cern.ch/jira/browse/CMSHLT-3147); \r\n- a couple of test to write and read the hits and tracks SoAs has been added;\r\n- to greatly simplify all the modules that downstream consume the SoAs and that are heavily templated just for the inputs (while doing exactly the same thing);\r\n- to integrate in an easier way non-pixel hits in the CA chain (e.g. strip hits);\r\n\r\n\r\n## Miscs\r\n\r\nI took the chance also to do some clean up here and there:\r\n- removing from the chain but leaving the definition in the code of the `AverageGeometry` actually never used. It can be easily re-enabled if needed;\r\n- a fix to `SimplePixelTopology` numbering for Phase2 modules that was affecting the cluster doublet cuts;\r\n- removed `idealConditions` flag that was changing the cluster cut based on the pixel barrel side (only for Run3). It has never been used and no beneficial effect was found ([studies were done in late 2021 and the efficiency was degradated](https://its.cern.ch/jira/browse/CMSHLT-2187)).\r\n- avoiding to have the `CPE` borught around in the chaing just for a single call to the `FrameSoA` that has been moved in the `CAGeometry`;\r\n- remove the limit to the number of vertices for HI conditions (set to `32` instead of standard `256`). This lead to crashes documented in https://github.com/cms-sw/cmssw/issues/46693. This would need anyway to be investigatged since, also in `master`, there are HI events (with noPU) for which we reconstruct >100 vertices.\r\n\r\n# Performance and Physics Studies\r\n\r\nNo changes to physics performance observed (as expected) for:\r\n\r\n- Run3 pixel-only tracking ([quads](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/run3_quads/plots_pixel_pixel/effandfakePtEtaPhi.png), [trips](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/run3_trips/));\r\n- [Run3 HLT](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/hlt_plots_pp_run3/);\r\n- Phase2 pixel-only tracking ([quads](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/phase2_quads/),[trips](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/phase2_trips/));\r\n- HI pixel-only tracking ([quads](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/hi_plots/)). \r\n\r\nSmall fluctuations visible for Phase2 and HI, I haven't found anything strange or a reason for them. They might be the \"usual\" irreproducibilities. \r\n\r\nPosting here a couple of examples for the records\r\n\r\n![image](https://github.com/user-attachments/assets/8dac8c40-bfd1-401b-9730-7ab30aade10e)\r\n\r\n![image](https://github.com/user-attachments/assets/9d5b9e71-39aa-4f69-8d50-6add4c0d60b6)\r\n\r\n----\r\n\r\n### pp HLT\r\n\r\nPerformance measured on `devfu-c2b03-44-01` running `/frozen/2024/2e34/v1.4/CMSSW_14_1_X/HLT` (adapted to be compatible with `15_0_0_pre2` and the PR) on ~10k events from `Run2024I`, Run`386593` and LS `94` `EphemeralHLTPhysics` data.\r\n\r\n----\r\n#### Througput and timing\r\n\r\nThe throughput is basically untouched\r\n\r\nthis PR:\r\n\r\n```\r\nRunning 3 times over 10000 events with 8 jobs, each with 32 threads, 24 streams and 1 GPUs\r\n   543.1    0.1 ev/s (9700 events, 98.1% overlap)\r\n   538.2    0.1 ev/s (9700 events, 99.0% overlap)\r\n   543.2    0.1 ev/s (9700 events, 97.4% overlap)\r\n --------------------\r\n   541.5    2.8 ev/s\r\n```\r\n\r\nmaster:\r\n\r\n```\r\nRunning 3 times over 10000 events with 8 jobs, each with 32 threads, 24 streams and 1 GPUs\r\n   539.9    0.1 ev/s (9700 events, 98.5% overlap)\r\n   540.6    0.1 ev/s (9700 events, 98.9% overlap)\r\n   540.7    0.1 ev/s (9700 events, 98.4% overlap)\r\n --------------------\r\n   540.4    0.5 ev/s\r\n```\r\n\r\nAs for the event timings find [here](https://adiflori.web.cern.ch/adiflori/circles/web/piechart.php?local=false&dataset=ca_geometry%2Fref_jsons%2Fpid1966878_ref&resource=time_real&colours=default&groups=hlt&data_name=data&show_labels=true&threshold=0) all the piecharts. Posting here two of them (for `real_time`) for the records. The average timings measured are (for the 8 jobs x 3 times):\r\n\r\n- master:  `488.48  4.08 ms/ev`\r\n- this PR: `487.72  4.40 ms/ev`\r\n\r\n----\r\n#### Memory\r\n\r\nAll the memory plots are under https://adriano.web.cern.ch/ca_geometry/memory/.\r\n\r\nThe memory usage for 8 jobs x 32 threads x 24 streams is reduced by ~47%. \r\n\r\n![hlt_pp_mem_32t24s8j](https://github.com/user-attachments/assets/634419c5-1221-4ed2-a793-9f5086f5846b)\r\n----\r\n\r\n### Phase2\r\n\r\nPerformance measured on a TTbar D110 PU Run4 RelVal sample (EDM input):\r\n\r\n- `/RelValTTbar_14TeV/CMSSW_15_0_0_pre1-PU_141X_mcRun4_realistic_v3_STD_Run4D110_PU-v1/GEN-SIM-DIGI-RAW`\r\n\r\n----\r\n\r\n#### Througput and timing\r\n\r\nThe optimal setup I found for `master` is 2 jobs with 8 threads and 8 streams. With 3 jobs we go out of memory and with 12 or 16 threads the througput is the same, just the memory increases. So I took this as the *baseline* for the comparisons. Here I'm running quadruplets only since for triplets the memory occupancy is very similar. This is a consequences of the fact that doublets related containers are vastly dominating and that, at the moment, they are the same for quads and trips given the same cell graph.\r\n\r\nthis PR:\r\n\r\n```\r\nRunning 3 times over 1300 events with 2 jobs, each with 8 threads, 8 streams and 1 GPUs\r\n    77.3    0.0 ev/s (1000 events, 99.6% overlap)\r\n    77.0    0.0 ev/s (1000 events, 99.6% overlap)\r\n    77.2    0.0 ev/s (1000 events, 99.5% overlap)\r\n --------------------\r\n    77.2    0.2 ev/s\r\n```\r\n\r\nmaster:\r\n\r\n```\r\nRunning 3 times over 1300 events with 2 jobs, each with 8 threads, 8 streams and 1 GPUs\r\n    70.1    0.1 ev/s (1000 events, 99.7% overlap)\r\n    69.4    0.1 ev/s (1000 events, 99.4% overlap)\r\n    69.2    0.1 ev/s (1000 events, 98.7% overlap)\r\n --------------------\r\n    69.6    0.5 ev/s  \r\n```\r\n\r\n----\r\n\r\n#### Memory\r\n\r\nIn terms of memory the effect is even more important that for Run3 HLT with a reduction of ~70% in a configuration with 2 jobs 8 threads and 8 streams (`8t8s2j`). \r\n\r\n![phase2_memory_throughput_quads_reference](https://github.com/user-attachments/assets/61607a91-a78f-4115-8f14-df4721b94bcf)\r\n\r\nThe same for `12t12s2j` that is not really beneficial for the througput and that is almost filling the two T4 available (for `master`). Also, having more memory available, configurations with more jobs may be tested brining almost a factor 2 to max throughput. \r\n\r\n![phase2_memory_throughput_quads](https://github.com/user-attachments/assets/07e1c0f9-04a2-45f5-b86c-d6b40dd8ad5b)\r\n\r\n(note maybe there's some further room for improvement using the euristical sizes)\r\n \r\n-----\r\n\r\n### HIon\r\n\r\n(thanks to Soohwan for the informations and the samples to set this up)\r\n\r\nAt the moment the HIon menu runs only the pixel local reconstruction on GPU since the pixel track reco is too heavy on the GPU memory. The performance here are mesured:\r\n- on MinBias events from `/store/hidata/HIRun2024B/HIEphemeralHLTPhysics/RAW/v1/000/388/305/00000/d8b13b7d-a94e-4b1f-9aae-bd86836a0459.root`, converted to raw; \r\n- with `/dev/CMSSW_14_2_0/HIon/V11` menu:\r\n    - as is in master (`RefCpu`);\r\n    - as is in master turning on the GPU pixel track reco in Alpaka (`Ref`);\r\n    - modified for this PR with the pixel track reco running on GPU in Alpaka and the max number of cells fixed to the current threshold (`Dev`).\r\n\r\nIf I understood well and the configuration stayed the same we currently run with 8 jobs 8 threads and 8 streams (see https://its.cern.ch/jira/browse/CMSHLT-2951). For a full GPU menu (`Ref`) the best I could fit in two T4 is a setup with `8t8s2j`. Running the same setup with this PR the memory usage is reduced by ~72% (with a +70% in througput). \r\n\r\n![hlt_hion_memory_throughput_fixed](https://github.com/user-attachments/assets/ba143949-82a8-4866-9811-b5e8d633e211)\r\n\r\nGiven the lighter memory footprint we can push a bit the full GPU HI menu reaching the same througput w.r.t. to the `RefCpu` setup (`16j16s8j`) with `12t12s4j`. And can go up to +240% it with `16t16s8j` (the maximum I could get). \r\n\r\n![hlt_hion_memory_throughput_fixed_best](https://github.com/user-attachments/assets/ac170ca0-9186-4d41-9408-a0dbedbf9b74)\r\n\r\nThe HI runs seems also a good candidate to test the euristical sizes. For example or run `HIRun2024B`, `Run=388305` and `LS=123` `EphemeralHLTPhysics` data, if we plot the number of cells, hits or tracks from consecutive events, we see:\r\n\r\n![run388305_HIEphemeralHLTPhysics_stats](https://github.com/user-attachments/assets/bc0a159f-4d59-4f84-96c0-c800bbdc8db9)\r\n\r\nFor cells we go from 1e4 to 1e6 (for non zero values). And we can fit the number of cells vs the number of hits:\r\n\r\n![hlt_hionVsHits](https://github.com/user-attachments/assets/02070189-0eaa-4dbb-911c-abdfd57811fb)\r\n\r\nUsing this cut for cells we can reach up to more than three times the current througput with `16t16s16j` (while keeping a good margin on the max memory available). Going above (with e.g. `16t16s20j`) is just increasing the memory occupation while keeping the same througput. \r\n\r\n![image](https://github.com/user-attachments/assets/e259d39e-a207-4fdf-bcaf-6d603cc500a6)\r\n\r\n\r\n## 26/05/2025 Update\r\n\r\nPosting here some new results with the latest updates.\r\n\r\n### Physics\r\nThe links have been updated with the latest results:\r\n\r\n- Run3 pixel-only tracking ([quads](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/run3_quads/plots_pixel_pixel/effandfakePtEtaPhi.png), [trips](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/run3_trips/));\r\n- [Run3 HLT](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/hlt_plots_pp_run3/);\r\n- Phase2 pixel-only tracking ([quads](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/phase2_quads/),[trips](https://adiflori.web.cern.ch/adiflori/ca_geometry_pr/mtv_plots/phase2_trips/));\r\n\r\nBasically on the spot, with some minor fluctuations visible. \r\n\r\n### Memory & Throughput\r\n\r\nAll tests done on `devfu-c2b03-44-01` ( so 2 x T4 + AMD EPYC 7763).\r\n\r\n### Run3 HLT pp\r\n\r\nInputs: `/gpu_data/store/data/Run2024I/EphemeralHLTPhysics/FED/run386593/`.\r\n\r\nIn `master`:\r\n\r\n```\r\nRunning 3 times over 10000 events with 8 jobs, each with 32 threads, 24 streams and 1 GPUs\r\n   533.0    0.1 ev/s (9700 events, 99.1% overlap)\r\n   532.0    0.1 ev/s (9700 events, 99.2% overlap)\r\n   530.4    0.1 ev/s (9700 events, 98.6% overlap)\r\n --------------------\r\n   531.8    1.3 ev/s\r\n```\r\n\r\nand with this PR:\r\n\r\n```\r\nRunning 3 times over 10000 events with 8 jobs, each with 32 threads, 24 streams and 1 GPUs\r\n   533.7    0.1 ev/s (9700 events, 99.2% overlap)\r\n   533.4    0.1 ev/s (9700 events, 98.9% overlap)\r\n   536.4    0.1 ev/s (9700 events, 98.9% overlap)\r\n --------------------\r\n   534.5    1.7 ev/s\r\n```\r\n\r\nso no major difference between the two. In terms of memory the ~50% reduction is confirmed. The two trends to be compared here are the orange (`this PR`)  and the blue (`master`):\r\n\r\n![hltpp_memory_throughput-1](https://github.com/user-attachments/assets/60e5ae6b-c34b-423b-acea-a541f7e2447f)\r\n\r\nI've added other three trends to check the effect of two changes:\r\n\r\n- replacing the `FillHitsModuleStart` with the simple `multiBlockPrefixScan` ([diff here](https://github.com/cms-sw/cmssw/pull/47611/files#diff-f95e465776106a3d31c221b38bb55d0555394cb8dcf9ed93b8975dca4b816598)). The comparison here is orange (`this PR`) and red (`this PR + no pref.`).  No effect on the throughput, minor effect on the memory (taking into account the fluctuations between the three runs).\r\n- changing the `CAFishbone` from precomputing some cells parameters before the inner loops in few fixed sized array to have it doing these calculations on the fly in the inner loop ([diff here](https://github.com/cms-sw/cmssw/pull/47611/files#diff-147206372fbd9494e3549aa4630d1aad1a2db3e9291e1072e2d874e3e79df618)). The comparison here is orange (`this PR`) and violet (`this PR + no flex.`). No major effect on the throughput is visible while a -15% in memory consumption is there.\r\n\r\n(in addition in green the setup is `this PR` without both the changes to the fishbone and the `FillHitsModuleStart` kernel)\r\n\r\n### Phase2 Quadruplets Pixel Tracks\r\n\r\nN.B. in order to have a fair comparison the `master` runs here are done including the changes to `SimplePixelTopology` to fix [the bug for the layer start](https://github.com/cms-sw/cmssw/pull/47611/files#diff-410ff88ea4b374b0f2760c72bf8faf437ca02d653b173fd00d9305b99e8fcec0).\r\n\r\nInputs: `/relval/CMSSW_15_0_0_pre2/RelValTTbar_14TeV/GEN-SIM-DIGI-RAW/PU_141X_mcRun4_realistic_v3_STD_Run4D110_PU-v1/`\r\n\r\nIn master\r\n\r\n```\r\nRunning 3 times over 1300 events with 2 jobs, each with 8 threads, 8 streams and 1 GPUs\r\n    69.9    0.1 ev/s (1000 events, 98.3% overlap)\r\n    70.1    0.1 ev/s (1000 events, 98.9% overlap)\r\n    70.0    0.1 ev/s (1000 events, 99.0% overlap)\r\n --------------------\r\n    70.0    0.1 ev/s\r\n```\r\n\r\nwith this PR\r\n\r\n```\r\nRunning 3 times over 10000 events with 2 jobs, each with 8 threads, 8 streams and 1 GPUs\r\n    77.2    0.0 ev/s (1100 events, 97.5% overlap)\r\n    77.4    0.0 ev/s (1100 events, 98.0% overlap)\r\n    77.7    0.0 ev/s (1100 events, 97.7% overlap)\r\n --------------------\r\n    77.4    0.2 ev/s\r\n```\r\n\r\nso, confirmed the ~10%  increase seen before.\r\n\r\nAnd also the ~70% decrease in memory usage:\r\n \r\n![phase2_memory_throughput_quads_reference](https://github.com/user-attachments/assets/b550c77e-ea77-4977-91c2-7a0841c0c2f7)\r\n\r\n\r\n### Phase2 NGT\r\n\r\nHere I add also some tests done running `HLT:NGTScouting` on the same inputs as above. In this case the througput is very low so it's not really telling a lot. But it's a sanity check.\r\n\r\nIn `master`:\r\n\r\n```\r\nRunning 3 times over 10000 events with 2 jobs, each with 8 threads, 8 streams and 1 GPUs\r\n     3.0    0.0 ev/s (1100 events, 98.9% overlap)\r\n     3.0    0.0 ev/s (1100 events, 99.0% overlap)\r\n     3.0    0.0 ev/s (1100 events, 99.6% overlap)\r\n --------------------\r\n     3.0    0.0 ev/\r\n```\r\n\r\nwith this PR\r\n\r\n```\r\nRunning 3 times over 10000 events with 2 jobs, each with 8 threads, 8 streams and 1 GPUs\r\n     3.0    0.0 ev/s (1100 events, 99.5% overlap) \r\n     3.0    0.0 ev/s (1100 events, 99.3% overlap) \r\n     3.0    0.0 ev/s (1100 events, 99.4% overlap) \r\n --------------------\r\n     3.0    0.0 ev/s \r\n```\r\n\r\nSo: low and untouched. For the memory we achieve ~58% reduction on the whole menu:\r\n\r\n![hlt_phase2_memory_throughput_fixed](https://github.com/user-attachments/assets/682c6732-065f-49b6-bbfb-9b6122f039a7)\r\n", "branch": "master", "changed_files": 96, "comments": 123, "commits": 1, "created_at": "1742211150", "deletions": 2554, "labels": ["reconstruction-pending", "dqm-pending", "hlt-pending", "geometry-pending", "pending-signatures", "tests-pending", "orp-pending", "code-checks-pending", "heterogeneous-pending", "tracking", "trk", "changes-dataformats"], "milestone": "CMSSW_15_1_X", "number": 47611, "release-notes": [], "review_comments": 182, "state": "open", "title": "A More Flexible And Lightweight CA", "updated_at": "1750415402", "user": "AdrianoDee"}