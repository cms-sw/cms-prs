{"additions": 976, "auther_ref": "useAccelerators_v2", "auther_sha": "e0d39f3428b026249f8097a60a6cb3fd82b47261", "author": "makortel", "body": "#### PR description:\r\n\r\nThis PR resolves #31760. It adds a new parameter\r\n```py\r\nprocess.options.accelerators = cms.untracked.vstring('*')\r\n```\r\n~~that can be used to specify the compute accelerator(s) a job should use. A special value `auto` (also the default to preserve the current behavior with CUDA) can be used to let the job to pick the accelerators that are available on a worker node. An empty `vstring` means that no accelerators are used. If a specific accelerator name is given and the worker node does not have that accelerator, the job is terminated with a specific exit code.~~\r\nthat can be used to specify the compute accelerator(s) a job is allowed to use. Patterns with `*` and `?` wildcards are allowed (similar to shell). Default value is `*` (i.e. the intersection what's defined in the job and available in the worker node) to preserve the current behavior with CUDA. An empty `vstring` is an error.\r\n\r\nThe recognized (and allowed) values are specified with instances of new `ProcessAccelerator`-derived classes whose objects are attached to the `Process`. The system implicitly adds `cpu` label to denote CPU fallback. Each `ProcessAccelerator` class defines the accelerator labels it recognizes, returns a subset of the labels that are enabled on a worker node, and have a possibility to customize (within restrictions) the `Process` right before the point where the python configuration is \"serialized\" for C++ code at the worker node. These customizations must not change the configuration hash.\r\n\r\nIn order to interoperate with `ProcessAccelerator` the `SwitchProducer`s are changed slightly: the case-specific functions that tell whether that case is enabled or not take now the list labels of enabled accelerators as an argument.\r\n\r\nEncapsulating the accelerator knowledge into specific configuration objects defined outside of the framework leaves the framework to stay independent of the accelerator technologies, and makes the system flexible and easy to extend. This design should be easy to extend e.g. for the use of Alpaka (or any portability technology) that can support multiple accelerators and communicate additional information about the chosen accelerator for the framework (such as the namespace of EDModules corresponding the chosen accelerator). It should also help with https://github.com/cms-sw/cmssw/issues/30044.\r\n\r\nFor CUDA this PR adds `ProcessAcceleratorCUDA`, and changes all current loads of `HeterogeneousCore.CUDAServices.CUDAService_cfi` with `HeterogeneousCore.CUDACore.ProcessAcceleratorCUDA_cfi`. The `ProcessAcceleratorCUDA` internally loads the `CUDAService` if it is not loaded already, and also adds the `CUDAService` to MessageLogger categories. The logic of whether CUDA is enabled on a worker node or not is moved from `SwitchProducerCUDA` to `ProcessAcceleratorCUDA`. For the NVIDIA GPU \"acclerator label\" I picked `gpu-nvidia`.\r\n\r\n#### PR validation:\r\n\r\nUnit tests pass (on a machine without a GPU and on a machine with a GPU)", "branch": "master", "changed_files": 41, "comments": 42, "commits": 9, "created_at": "1642019790", "deletions": 107, "labels": ["reconstruction-pending", "dqm-pending", "hlt-pending", "core-pending", "operations-pending", "pending-signatures", "orp-pending", "tests-started", "upgrade-pending", "code-checks-approved", "heterogeneous-pending"], "milestone": "CMSSW_12_3_X", "number": 36699, "release-notes": [], "review_comments": 27, "state": "open", "title": "Add a generic mechanism to specify compute accelerators to use in the configuration", "updated_at": "1645453153", "user": "makortel"}