{"additions": 4934, "auther_ref": "torch-alpaka-pr", "auther_sha": "b81a950db160fc1c86508137be5615a427395bd2", "author": "lukaszmichalskii", "body": "#### PR description:\r\n\r\nThis PR enables seamless integration between PyTorch and the Alpaka-based heterogeneous computing backend, supporting inference workflows with usage of `pytorch` library with `PortableCollection`s objects. It provides:\r\n- Compatibility with Alpaka device/queue abstractions.\r\n- Support for automatic conversion of optimized SoA to torch tensors, with memory blob reusage.\r\n- Support for just-in-time (JIT) model execution (with some proof-of-concept ahead-of-time (AOT) solution).\r\n- Single-threading and CUDA stream management are handled by `QueueGuard` objects specialized for each supported backend.\r\n\r\nThis implementation was presented and discussed at:\r\n- Core Software Meeting: \r\n   - general idea: https://indico.cern.ch/event/1538634/#17-pytorch-integration-in-cmss, \r\n   - conversion concept: https://indico.cern.ch/event/1538634/#18-user-friendly-integration-o\r\n   - AOT integration discussion: https://indico.cern.ch/event/1572252/#16-integrating-pytorch-in-alpa\r\n- GPU Developments Meeting: \r\n   - heterogeneous design: https://indico.cern.ch/event/1566840/#2-pythonalpaka-developments-in\r\n\r\n#### PR validation:\r\n\r\nIncluded demonstration code of interoperability between `SoA` constructs with `PyTorch` C++ API and CMSSW environment in [PyTorchAlpakaTest](https://github.com/cms-sw/cmssw/tree/61f7f887b822b4ac6f8457b5133480e5d16cc744/PhysicsTools/PyTorchAlpakaTest) package.\r\n\r\n### PyTorch Ahead-of-time compilation\r\n\r\nThis pull request also investigates AOT compilation strategy but is in beta version (proof of concept) not yet ready for production usage.\r\n\r\n### GPU support\r\nCUDA backend is supported and tested, ROCm is not yet supported: [cms-sw/cmsdist#9786](https://github.com/cms-sw/cmsdist/pull/9786). To ensure pipelines running on AMD nodes are not left without inference capability, the CPU fallback is implemented. This fallback transfers inference data to the host (explicitly synchronizes it with `alpaka::wait()`), executes inference on the CPU, and then copies the results back to the output buffer.\r\n\r\nFYI @valsdav @ericcano @felicepantaleo @chrisizeh @leobeltra", "branch": "master", "changed_files": 80, "comments": 78, "commits": 3, "created_at": "1745997609", "deletions": 408, "labels": ["pending-signatures", "tests-pending", "orp-pending", "new-package-pending", "code-checks-approved", "heterogeneous-rejected", "ml-pending", "changes-dataformats"], "milestone": "CMSSW_16_0_X", "number": 47984, "release-notes": [], "review_comments": 192, "state": "open", "title": "Integrating PyTorch in Alpaka heterogeneous core", "updated_at": "1760604533", "user": "lukaszmichalskii"}